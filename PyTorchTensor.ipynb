{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f10e12",
   "metadata": {
    "id": "d3f10e12"
   },
   "source": [
    "## PyTorch tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c35986",
   "metadata": {
    "id": "94c35986"
   },
   "source": [
    "##### A tensor is a “n dimensional array” (0 to n dimension). Scalars, vectors, and matrices are examples of tensors.\n",
    "* scalar: 0 dimension --> point --> rank 0 tensor\n",
    "* vector: 1 dimension --> line --> rank 1 tensor\n",
    "* matrix: 2 dimension --> plane --> rank 2 tensor\n",
    "* tensor: n dimensional array --> rank n tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205bf2c6",
   "metadata": {
    "id": "205bf2c6"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "teixyMvKhnJw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "teixyMvKhnJw",
    "outputId": "957745b5-cdf0-482d-b047-80fddd5748e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0+cu118\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ee08a8",
   "metadata": {
    "id": "e5ee08a8"
   },
   "source": [
    "### PyTorch.tensor data types\n",
    "\n",
    "Data is stored as numbers with default data type being floating point numbers. PyTorch supports \n",
    "several numerical data types. Data types can be defined basically, using one of the three following different ways:\n",
    "\n",
    "1) At creation time by providing the dtype argument to the constructor method,\n",
    "\n",
    "2) by calling the `torch.Tensor.to(device=None, dtype=None)` method and providing the dtype argument,\n",
    "\n",
    "3) or through casting functions. e.g.: `torch.Tensor.double(memory_format=torch.preserve_format)`; `Tensor.bool(memory_format=torch.preserve_format)`; `Tensor.int(memory_format=torch.preserve_format)`; ecc.\n",
    "\n",
    "Call `torch.get_default_dtype()` to find out what is the default data type.\n",
    "\n",
    "And `torch.can_cast(from, to)` to find out if the type conversion is allowed under PyTorch casting rules. (https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2008e435",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2008e435",
    "outputId": "f505a47b-1def-4c2c-d4a9-c68e6031a5a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default dtype: torch.float32\n",
      "\n",
      "\n",
      "tensor([[[ True],\n",
      "         [ True],\n",
      "         [ True],\n",
      "         [False]]])\n",
      "torch.bool\n",
      "\n",
      "\n",
      "tensor([[[1],\n",
      "         [1],\n",
      "         [1],\n",
      "         [0]]], dtype=torch.int16)\n",
      "torch.int16\n",
      "\n",
      "\n",
      "tensor([[[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [0.]]])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# print default dtype\n",
    "print(f'default dtype: {torch.get_default_dtype()}')\n",
    "\n",
    "print('\\n')\n",
    "A = torch.tensor([[[True], [True], [True], [False]]], dtype=torch.bool)\n",
    "print(A)\n",
    "print(A.dtype)\n",
    "\n",
    "print('\\n')\n",
    "B = A.to(torch.int16)\n",
    "print(B)\n",
    "print(B.dtype)\n",
    "\n",
    "print('\\n')\n",
    "C = B.float()\n",
    "print(C)\n",
    "print(C.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9Vp5momFe8lH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Vp5momFe8lH",
    "outputId": "f1c0b1a3-4bb2-4bb1-9f73-f98fa3c51b64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.can_cast(torch.int, torch.float))\n",
    "print(torch.can_cast(torch.float, torch.double))\n",
    "print(torch.can_cast(torch.double, torch.float))\n",
    "\n",
    "print(torch.can_cast(torch.float, torch.int))\n",
    "print(torch.can_cast(torch.float, torch.bool))\n",
    "print(torch.can_cast(torch.int, torch.bool))\n",
    "\n",
    "print(torch.can_cast(torch.bool, torch.int))\n",
    "print(torch.can_cast(torch.bool, torch.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9a40e9",
   "metadata": {
    "id": "fd9a40e9"
   },
   "source": [
    "### Creation ops (Constructors)\n",
    "Creation ops are methods used to instantiate a tensor.\n",
    "\n",
    "To learn more about creation ops: https://pytorch.org/docs/stable/torch.html#tensor-creation-ops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f9a4b7",
   "metadata": {
    "id": "a0f9a4b7"
   },
   "source": [
    "* To create a tensor with pre-existing data\n",
    "\n",
    "`torch.tensor(data, dtype=None, device=None, requires_grad=False, pin_memory=False)` -- Constructs a leaf tensor (tensor with no autograd history)\n",
    "\n",
    "* To create a tensor with a specific size filled with initialized, memory, data\n",
    "\n",
    "`torch.empty(size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False, memory_format=torch.contiguous_format)`\n",
    "\n",
    "* To create a tensor with specific size filled with a scalar\n",
    "\n",
    "`torch.zeros(size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)`\n",
    "\n",
    "`torch.ones(size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)`\n",
    "\n",
    "`torch.rand(size, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False)`\n",
    "\n",
    "`torch.full(size, fill_value, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)`\n",
    "\n",
    "* Others possibilities:\n",
    "\n",
    "`torch.as_tensor()` -- Preserves autograd history and avoids copies where possible.\n",
    "\n",
    "`torch.from_numpy()` -- Creates a tensor that shares storage with a NumPy array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cf9aed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9cf9aed",
    "outputId": "3ccc017c-d5bc-4781-f27b-a7125124028b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 4., 3., 2., 5., 1.])\n",
      "tensor([[2., 4.],\n",
      "        [1., 5.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1.0, 4.0, 3.0, 2.0, 5.0, 1.0])\n",
    "b = torch.tensor([[2.0, 4.0], [1.0, 5.0]])\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc6277e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfc6277e",
    "outputId": "ea6d3e50-9e6c-4cc7-995f-76ee1b7c653d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1383e-16,  4.5771e-41,  1.0861e-34,  0.0000e+00, -5.4542e+19,\n",
      "          4.5769e-41]]) \n",
      " tensor([[0., 0.],\n",
      "        [0., 0.]]) \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      " tensor([[0.8105, 0.3995, 0.7601],\n",
      "        [0.7168, 0.2727, 0.1475]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.empty(1, 6) # return a tensor filled with unitialized data\n",
    "b = torch.zeros(2, 2) # filled with zeros\n",
    "c = torch.ones(2, 3) # filled with ones\n",
    "d = torch.rand(2, 3) # filled with random values\n",
    "print(a, '\\n', b, '\\n', c, '\\n', d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6bbeda",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2b6bbeda",
    "outputId": "8ee9f834-666c-4632-e51c-1cf0244d25be",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[13, 13],\n",
      "        [13, 13],\n",
      "        [13, 13]])\n"
     ]
    }
   ],
   "source": [
    "g = torch.full((3,2), fill_value = 13)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6af3fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa6af3fd",
    "outputId": "de15761e-e42e-45ad-9c67-1421103e1f13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 7, 8],\n",
      "        [1, 3, 4]])\n",
      "\n",
      "<class 'numpy.ndarray'>\n",
      "[13. 15. 11.]\n",
      "\n",
      "<class 'torch.Tensor'>\n",
      "tensor([13., 15., 11.], dtype=torch.float64)\n",
      "\n",
      "'Y_array' and 'Y_tensor' share same storage: True\n"
     ]
    }
   ],
   "source": [
    "# torch.as_tensor(data, dtype=None, device=None) -- Preserves autograd history and avoids copies where possible\n",
    "# data argument can be a list, tuple, NumPy ndarray, scalar, and other types\n",
    "\n",
    "X = torch.as_tensor([[5,7,8],[1,3,4]])\n",
    "print(X)\n",
    "\n",
    "import numpy as np\n",
    "Y_array = np.array([13., 15., 11.])\n",
    "print(f'\\n{type(Y_array)}\\n{Y_array}')\n",
    "\n",
    "# torch.from_numpy(ndarray) -- Creates a tensor that shares storage with a NumPy array\n",
    "# Limitations: Modifications to the tensor will be reflected in the ndarray and vice versa. The returned tensor is not resizable.\n",
    "\n",
    "Y_tensor = torch.from_numpy(Y_array)\n",
    "print(f'\\n{type(Y_tensor)}\\n{Y_tensor}')\n",
    "\n",
    "# use torch.Tensor.data_ptr() method to check if the memory allocations matchs\n",
    "print(f'\\n\\'Y_array\\' and \\'Y_tensor\\' share same storage: {Y_tensor[0].data_ptr() == Y_tensor[0].data_ptr()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b70e09",
   "metadata": {
    "id": "39b70e09"
   },
   "source": [
    "#### `torch.*_like()` methods\n",
    "methods with `*_like` are used to reproduce the shape of the input tensor. They return a tensor of same size as input tensor filled with scalar value 0 (`torch.zeros_like(input, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format)`) or 1 (`torch.ones_like(input, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d03cb3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4d03cb3",
    "outputId": "b2e8bc68-8471-4834-c12b-7077b47d99b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0.]])\n",
      "'x' and 'a' share same storage: False\n",
      "\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "'y' and 'b' share same storage: False\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros_like(a)\n",
    "print(x)\n",
    "# the returned tensor is not a view of the input tensor\n",
    "print(f'\\'x\\' and \\'a\\' share same storage: {x[0].data_ptr() == a[0].data_ptr()}\\n')\n",
    "\n",
    "y = torch.ones_like(b)\n",
    "print(y)\n",
    "# the returned tensor is not a view of the input tensor\n",
    "print(f'\\'y\\' and \\'b\\' share same storage: {y[0].data_ptr() == b[0].data_ptr()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b708aa",
   "metadata": {
    "id": "30b708aa"
   },
   "source": [
    "### Tensor Attributes, Properties, and Built-in methods\n",
    "\n",
    "#### attributes\n",
    "`torch.dtype`\n",
    "\n",
    "`torch.device`\n",
    "\n",
    "`torch.layout`\n",
    "\n",
    "`torch.Tensor.grad`\n",
    "\n",
    "`torch.Tensor.requires_grad`\n",
    "\n",
    "#### properties\n",
    "`torch.Tensor.storage()`\n",
    "\n",
    "`torch.Tensor.shape`\n",
    "\n",
    "#### built-in methods\n",
    "`torch.Tensor.size(dim=None)`\n",
    "\n",
    "`torch.Tensor.storage_offset()`\n",
    "\n",
    "`torch.Tensor.stride(dim)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swBDFoMUKLWf",
   "metadata": {
    "id": "swBDFoMUKLWf"
   },
   "outputs": [],
   "source": [
    "y = 2\n",
    "a = torch.as_tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BdrDBv49Iwz9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BdrDBv49Iwz9",
    "outputId": "c308f7c3-35dd-4a78-f6e8-1e7e5f475844"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype: torch.int64; device: cpu\n",
      "\n",
      "Storage:\n",
      " 2\n",
      "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 1]\n",
      "\n",
      "Shape: torch.Size([])\n",
      "\n",
      "size: torch.Size([]); storage_offset: 0; stride: ()\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-237c728b85d6>:3: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  print(f'Storage:\\n{a.storage()}\\n')\n"
     ]
    }
   ],
   "source": [
    "print(f'dtype: {a.dtype}; device: {a.device}\\n')\n",
    "\n",
    "print(f'Storage:\\n{a.storage()}\\n')\n",
    "\n",
    "print(f'Shape: {a.shape}\\n')\n",
    "\n",
    "print(f'size: {a.size()}; storage_offset: {a.storage_offset()}; stride: {a.stride()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uhnrPnzCvA3-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uhnrPnzCvA3-",
    "outputId": "16ba0db5-3c2d-43b7-cfe9-7db9190a3e49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype: torch.float32; device: cpu; layout: torch.strided\n",
      "\n",
      "grad: None; requires_grad: False\n",
      "\n",
      "Storage:\n",
      " 0.34831613302230835\n",
      " 0.21174710988998413\n",
      " 0.8232155442237854\n",
      " 0.8076144456863403\n",
      " 0.9091317057609558\n",
      " 0.3293258547782898\n",
      "[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6]\n",
      "\n",
      "Shape: torch.Size([2, 3])\n",
      "\n",
      "size: torch.Size([2, 3]); storage_offset: 0; stride: (3, 1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-681253f7f499>:6: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  print(f'Storage:\\n{x.storage()}\\n')\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 3)\n",
    "print(f'dtype: {x.dtype}; device: {x.device}; layout: {x.layout}\\n')\n",
    "\n",
    "print(f'grad: {x.grad}; requires_grad: {x.requires_grad}\\n')\n",
    "\n",
    "print(f'Storage:\\n{x.storage()}\\n')\n",
    "\n",
    "print(f'Shape: {x.shape}\\n')\n",
    "\n",
    "print(f'size: {x.size()}; storage_offset: {x.storage_offset()}; stride: {x.stride()}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1d304a",
   "metadata": {
    "id": "5a1d304a"
   },
   "source": [
    "### Storage, size, stride, and storage offset\n",
    "\n",
    "The `storage` is the memory allocated to store the tensor data. It’s a contiguous block of memory by default. Every tensor has an attribute storage which holds its data. Several tensors can have the same storage.\n",
    "\n",
    "The tensor `size` is determined by the number of elements within each dimension. For instance a 2-D tensor has size defined by its number of rows (i) and its number of columns (j), i.e `2d_tensor.size() = tensor(i,j)`.\n",
    "\n",
    "The tensor `stride` is the number of locations in the storage between the elements at the beginning of successive arrays + 1 along each dimension (e.g. rows or columns in a 2-D tensor).\n",
    "\n",
    "The `storage_offset` of a tensor is the corresponding storage index of the first element of the tensor.\n",
    "Use the method `torch.as_strided(input, size, stride, storage_offset=None)` to modify a tensor’s size, stride, and storage_offset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c56407",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4c56407",
    "outputId": "ea756405-ce37-47fe-f075-39a804e7ecc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storage:\n",
      " 7.0\n",
      " 4.0\n",
      " 5.0\n",
      " 2.0\n",
      " 5.0\n",
      " 1.0\n",
      "[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6]\n",
      "\n",
      "Storage Memory address:\n",
      "140435363776736\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-05a037308aa9>:2: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  print(f'Storage:\\n{w.storage()}\\n')\n",
      "<ipython-input-11-05a037308aa9>:3: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  print(f'Storage Memory address:\\n{id(w.storage())}\\n')\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([7.0, 4.0, 5.0, 2.0, 5.0, 1.0])\n",
    "print(f'Storage:\\n{w.storage()}\\n')\n",
    "print(f'Storage Memory address:\\n{id(w.storage())}\\n')\n",
    "#print(f'Storage Memory address:\\n{id(w.untyped_storage())}') #use untyped_storage in place of storage to avoid warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nq7o_iwUz7xT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nq7o_iwUz7xT",
    "outputId": "a59d10ac-33cc-44cd-8367-e4f40f7c4224"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Address of first element in Storage:\n",
      "116384384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Address of first element in Storage:\\n{w.untyped_storage().data_ptr()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ab293d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89ab293d",
    "outputId": "72b1eb84-d180-4a71-f84b-331bc091e261"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original tensor:\n",
      "tensor([7., 4., 5., 2., 5., 1.])\n",
      "Size: torch.Size([6])\n",
      "Size: torch.Size([6])\n",
      "Storage_offset: 0\n",
      "Stride: (1,)\n",
      "\n",
      "r view:\n",
      "tensor([[4., 2.],\n",
      "        [2., 1.]])\n",
      "Size: torch.Size([2, 2])\n",
      "Shape: torch.Size([2, 2])\n",
      "Storage_offset: 1\n",
      "Stride: (2, 2)\n",
      "'r' and 'w' share same storage: True\n",
      "\n",
      "k view:\n",
      "tensor([[4., 5., 2.],\n",
      "        [2., 5., 1.]])\n",
      "Size: torch.Size([2, 3])\n",
      "Size: torch.Size([2, 3])\n",
      "Storage_offset: 1\n",
      "Stride: (2, 1)\n",
      "'k' and 'w' share same storage: True\n",
      "\n",
      "z view:\n",
      "tensor([[5., 2.],\n",
      "        [5., 1.]])\n",
      "Size: torch.Size([2, 2])\n",
      "Shape: torch.Size([2, 2])\n",
      "Storage_offset: 2\n",
      "Stride: (2, 1)\n",
      "'z' and 'w' share same storage: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# original tensor\n",
    "print(f'original tensor:\\n{w}')\n",
    "print(f'Size: {w.size()}')\n",
    "print(f'Size: {w.shape}')\n",
    "print(f'Storage_offset: {w.storage_offset()}')\n",
    "print(f'Stride: {w.stride()}\\n')\n",
    "\n",
    "# r view\n",
    "# change storage_offset with torch.as_strided()\n",
    "r = torch.as_strided(w, (2,2), (2,2), 1)\n",
    "print(f'r view:\\n{r}')\n",
    "print(f'Size: {r.size()}')\n",
    "print(f'Shape: {r.shape}')\n",
    "print(f'Storage_offset: {r.storage_offset()}')\n",
    "print(f'Stride: {r.stride()}')\n",
    "# use torch.Tensor.data_ptr() method to verify that if new tensor share same storage with original tensor\n",
    "print(f'\\'r\\' and \\'w\\' share same storage: {r.untyped_storage().data_ptr() == w.untyped_storage().data_ptr()}\\n')\n",
    "\n",
    "# k view\n",
    "# change storage_offset with torch.as_strided()\n",
    "k = torch.as_strided(w, (2,3), (2,1), 1)\n",
    "print(f'k view:\\n{k}')\n",
    "print(f'Size: {k.size()}')\n",
    "print(f'Size: {k.shape}')\n",
    "print(f'Storage_offset: {k.storage_offset()}')\n",
    "print(f'Stride: {k.stride()}')\n",
    "# use torch.Tensor.data_ptr() method to verify that new tensor share same storage with original tensor\n",
    "print(f'\\'k\\' and \\'w\\' share same storage: {k.untyped_storage().data_ptr() == w.untyped_storage().data_ptr()}\\n')\n",
    "\n",
    "# z view\n",
    "# change storage_offset with torch.as_strided()\n",
    "z = torch.as_strided(w, (2,2), (2,1), 2)\n",
    "print(f'z view:\\n{z}')\n",
    "print(f'Size: {z.size()}')\n",
    "print(f'Shape: {z.shape}')\n",
    "print(f'Storage_offset: {z.storage_offset()}')\n",
    "print(f'Stride: {z.stride()}')\n",
    "# use torch.Tensor.data_ptr() method to verify that if new tensor share same storage with original tensor\n",
    "print(f'\\'z\\' and \\'w\\' share same storage: {z.untyped_storage().data_ptr() == w.untyped_storage().data_ptr()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VxC4HLDQm--6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VxC4HLDQm--6",
    "outputId": "ea9d14a9-df1a-4ad7-e253-2c603707a1e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size row vector: torch.Size([3])\n",
      "Size column vector: torch.Size([3, 1])\n",
      "\n",
      "\n",
      "Shape row vector: torch.Size([3])\n",
      "Shape column vector: torch.Size([3, 1])\n",
      "\n",
      "\n",
      "Stride row vector: (1,)\n",
      "Stride column vector: (1, 1)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note: row and column vectors differ in size, shape, and stride\n",
    "\n",
    "print(f'Size row vector: {torch.tensor([1, 2, 3]).size()}')\n",
    "print(f'Size column vector: {torch.tensor([[1], [2], [3]]).size()}')\n",
    "print('\\n')\n",
    "\n",
    "print(f'Shape row vector: {torch.tensor([1, 2, 3]).shape}')\n",
    "print(f'Shape column vector: {torch.tensor([[1], [2], [3]]).shape}')\n",
    "print('\\n')\n",
    "\n",
    "print(f'Stride row vector: {torch.tensor([1, 2, 3]).stride()}')\n",
    "print(f'Stride column vector: {torch.tensor([[1], [2], [3]]).stride()}')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef7ae58",
   "metadata": {
    "id": "0ef7ae58"
   },
   "source": [
    "### Indexing\n",
    "With indexing the tensor content can be accessed element by element or in chuncks, as well as modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iXcjUkAqsI6Z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iXcjUkAqsI6Z",
    "outputId": "7da82927-e31a-42c9-9d1f-17bcab104bd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n",
      "\n",
      "\n",
      "tensor([2., 4.])\n",
      "tensor([[2., 4.],\n",
      "        [1., 5.],\n",
      "        [6., 3.]])\n",
      "tensor([[1., 5.],\n",
      "        [6., 3.],\n",
      "        [8., 7.]])\n",
      "\n",
      "\n",
      "tensor([4., 5., 3., 7.])\n",
      "tensor([6., 3.])\n",
      "\n",
      "\n",
      "tensor([4., 5., 3., 7.])\n",
      "tensor([5., 3., 7.])\n",
      "tensor([6., 3.])\n",
      "tensor([[2., 4.],\n",
      "        [1., 5.]])\n"
     ]
    }
   ],
   "source": [
    "p = torch.tensor([[2., 4.], [1., 5.], [6., 3.], [8., 7.]])\n",
    "\n",
    "# grabing element\n",
    "print(p[0, 1])\n",
    "print('\\n')\n",
    "\n",
    "# grabing a row chunck\n",
    "print(p[0])\n",
    "print(p[:3]) # from row index 0 to row index 2\n",
    "print(p[1:]) # from row index 1 to the last row\n",
    "print('\\n')\n",
    "\n",
    "# grabing a column chunk\n",
    "print(p[:,1])\n",
    "print(p[2,:])\n",
    "print('\\n')\n",
    "\n",
    "# specifying row and column\n",
    "print(p[:,1]) # all row values of column index 1\n",
    "print(p[1:,1]) # values from row index 1 of column index 1\n",
    "print(p[2,:]) # all values in row index 2\n",
    "print(p[:2,:]) # values from first row till row index 1 of all columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0GbKYalX2myU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0GbKYalX2myU",
    "outputId": "1d28b18e-5d29-4213-ff69-83f659fbb3a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 5.1000],\n",
      "        [1.0000, 5.0000],\n",
      "        [6.0000, 3.0000],\n",
      "        [8.0000, 7.0000]])\n",
      "\n",
      "\n",
      "tensor([[2.0000, 5.1000],\n",
      "        [1.0000, 5.0000],\n",
      "        [2.0000, 7.0000],\n",
      "        [8.0000, 7.0000]])\n"
     ]
    }
   ],
   "source": [
    "# modifying an element and chunk of elements\n",
    "p[0, 1] = 5.1\n",
    "print(p)\n",
    "print('\\n')\n",
    "\n",
    "# modifying a chunk of elements\n",
    "p[2] = torch.tensor([2.0, 7.0])\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zj165jDYkRwL",
   "metadata": {
    "id": "zj165jDYkRwL"
   },
   "source": [
    "### View tensor\n",
    "A view tensor is a copy tensor that shares the same underlying storage (i.e., memory and data) with its original tensor. Several PyTorch tensors can share the same storage so that memory is allocated only once. Such feature allows for performing fast operations on tensors such as reshaping, slicing, and element-wise operations.\n",
    "\n",
    "It’s important to note that modifying the data in a view tensor changes the data in the original tensor as well, and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef72e971",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ef72e971",
    "outputId": "0f87425e-7782-4d35-f134-f288db1fe2f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor(3.1400)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "A = torch.rand(4, 4)\n",
    "B = A.view(2, 8)\n",
    "\n",
    "#torch.Tensor.data_ptr() returns the address of the first element of self tensor\n",
    "print(B.untyped_storage().data_ptr() == A.untyped_storage().data_ptr())  # `B` and `A` share the same underlying data.\n",
    "A[0][0] = 3.14\n",
    "print(B[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "axTc8N7UoyOL",
   "metadata": {
    "id": "axTc8N7UoyOL"
   },
   "source": [
    "### View ops\n",
    "Some PyTorch tensor operations return a view of the input tensor instead of returning a new tensor. Some view ops methods bellow:\n",
    "\n",
    "- `torch.Tensor.view(shape)`\n",
    "- `torch.as_strided(input, size, stride, storage_offset=None)`\n",
    "Create a view of an existing torch.Tensor input with specified size, stride and storage_offset\n",
    "- `torch.Tensor.detach()`\n",
    "Returns a new Tensor, detached from the current graph\n",
    "- `torch.narrow(input, dim, start, length)`\n",
    "Returns a new tensor that is a narrowed version of input tensor (https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow)\n",
    "- `torch.select(input, dim, index)`\n",
    "Returns a view of input tensor as a slice along specified dimension and index.\n",
    "- `torch.squeeze(input, dim=None)`\n",
    "Returns a tensor with all specified dimensions of input of size 1 removed.\n",
    "- `torch.unsqueeze(input, dim)`\n",
    "Returns a new tensor with a dimension of size one inserted at the specified position.\n",
    "- `torch.transpose(input, dim0, dim1)`\n",
    "Returns a tensor that is a transposed version of input. The given dimensions dim0 and dim1 are swapped.\n",
    "- `torch.t(input)`\n",
    "Returns transpose dimensions 0 and 1 of a tensor of 2 dimesion or less.\n",
    "- `torch.permute(torch.Tensor)`\n",
    "Returns a view of the input tensor with permuted dimension.\n",
    "- `torch.reshape(input, shape)`\n",
    "Returns a view of input tensor with specified shape. This method returns a view of input tensor if the shapes are compatible and a copy otherwise (same applies to torch.flatten() and torch.Tensor.reshape_as()).\n",
    "- `torch.split(torch.Tensor, split_size/split_sections, dim=0)`\n",
    "Returns chunks of the input tensor along the specified dimension. Each chunck is a view of original tensor.\n",
    "- `torch.chunk(input, chunks, dim=0)`\n",
    "Attempts to split a tensor into specified number of chunks. Each chunk is a view of the input tensor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58af7821",
   "metadata": {
    "id": "58af7821"
   },
   "source": [
    "### Contiguity issue\n",
    "\n",
    "By default PyTorch tensors are allocated in dense non-overlapping memory, i.e. `memory_format = torch.contiguous_format`. (With strides represented by values in decreasing order.) However, calling a view method on a contiguous tensor can potentially produce a non-contiguous tensor.\n",
    "\n",
    "Two important methods here:\n",
    "\n",
    "1) `torch.Tensor.is_contiguous(memory_format=torch.contiguous_format)`\n",
    "\n",
    "Returns True if self tensor is contiguous in memory in the order specified by memory format. Where `memory_format` parameter specifies memory allocation order. Default: `torch.contiguous_format`. Use this method to verify data contiguity.\n",
    "\n",
    "\n",
    "2) `torch.Tensor.contiguous(memory_format=torch.contiguous_format)`\n",
    "\n",
    "This method returns a contiguous in memory tensor containing the same data as self tensor in the specified memory format. Call this method to enforce copying data when self tensor is not contiguous. Warning: If self tensor is already in the specified memory format, this function returns the self tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PUFpqUCbkPN8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PUFpqUCbkPN8",
    "outputId": "7f34cf42-d005-482b-acaa-3cf8ecab8e65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b is contiguous: True\n",
      "\n",
      "t is contiguous: False\n",
      "\n",
      "z is contiguous: True\n",
      "\n",
      "b stride: (2, 1)\n",
      "t stride: (1, 2)\n",
      "z stride: (2, 1)\n",
      "\n",
      "'t' and 'b' share same storage: True\n",
      "'z' and 'b' share same storage: False\n",
      "'z' and 't' share same storage: False\n"
     ]
    }
   ],
   "source": [
    "# Example 1: torch.transpose(input, dim0, dim1)\n",
    "\n",
    "b = torch.tensor([[0, 1],[2, 3]])\n",
    "print(f'b is contiguous: {b.is_contiguous()}\\n')\n",
    "\n",
    "t = b.transpose(0, 1)  # `t` is a view of `b`\n",
    "# View tensors might be non-contiguous\n",
    "print(f't is contiguous: {t.is_contiguous()}\\n')\n",
    "\n",
    "# Call Tensor.contiguous() to get a contiguous tensor\n",
    "z = t.contiguous()\n",
    "print(f'z is contiguous: {z.is_contiguous()}\\n')\n",
    "\n",
    "# verify stride\n",
    "print(f'b stride: {b.stride()}')\n",
    "print(f't stride: {t.stride()}')\n",
    "print(f'z stride: {z.stride()}')\n",
    "\n",
    "# verify storage\n",
    "print(f'\\n\\'t\\' and \\'b\\' share same storage: {t.untyped_storage().data_ptr() == b.untyped_storage().data_ptr()}')\n",
    "print(f'\\'z\\' and \\'b\\' share same storage: {z.untyped_storage().data_ptr() == b.untyped_storage().data_ptr()}')\n",
    "print(f'\\'z\\' and \\'t\\' share same storage: {z.untyped_storage().data_ptr() == t.untyped_storage().data_ptr()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zAgeQjxqQTfn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zAgeQjxqQTfn",
    "outputId": "b62ae011-2162-4569-b9a0-8f05b1c89924"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor x\n",
      "tensor([[3., 6., 2.],\n",
      "        [4., 5., 9.]])\n",
      " stride: (3, 1)\n",
      " is contiguous: True\n",
      "\n",
      "tensor y\n",
      "tensor([[3., 4.],\n",
      "        [6., 5.],\n",
      "        [2., 9.]])\n",
      " stride: (1, 3)\n",
      " is contiguous: False\n",
      "\n",
      "tensor k\n",
      "tensor([[6., 2.],\n",
      "        [4., 5.]])\n",
      " stride: (2, 1)\n",
      " is contiguous: True\n",
      "\n",
      "tensor w\n",
      "tensor([[6., 2.],\n",
      "        [4., 5.]])\n",
      " stride: (2, 1)\n",
      " is contiguous: True\n",
      "\n",
      "'y' and 'x' share same storage: True\n",
      "'k' and 'x' share same storage: True\n",
      "'w' and 'k' share same storage: True\n",
      "'w' and 'x' share same storage: True\n",
      "\n",
      "tensor(0.5000)\n",
      "tensor(0.5000)\n"
     ]
    }
   ],
   "source": [
    "# Example 2: torch.t(input)\n",
    "\n",
    "print(f'tensor x')\n",
    "x = torch.tensor([[3.0, 6.0, 2.0], [4.0, 5.0, 9.0]])\n",
    "print(x)\n",
    "print(f' stride: {x.stride()}\\n is contiguous: {x.is_contiguous()}\\n')\n",
    "\n",
    "print(f'tensor y')\n",
    "y = torch.t(x)\n",
    "print(y)\n",
    "# View tensors might be non-contiguous\n",
    "print(f' stride: {y.stride()}\\n is contiguous: {y.is_contiguous()}\\n') # non-contiguous\n",
    "\n",
    "print(f'tensor k')\n",
    "k = torch.as_strided(x, (2, 2), (2,1), 1)\n",
    "print(k)\n",
    "# View tensors might be non-contiguous\n",
    "print(f' stride: {k.stride()}\\n is contiguous: {k.is_contiguous()}\\n') # contiguous\n",
    "\n",
    "print(f'tensor w')\n",
    "# calling torch.Tensor.contiguos() on a contiguous tensor returns the self tensor\n",
    "w = k.contiguous()\n",
    "print(w)\n",
    "print(f' stride: {w.stride()}\\n is contiguous: {w.is_contiguous()}\\n')\n",
    "\n",
    "# verify storage\n",
    "print(f'\\'y\\' and \\'x\\' share same storage: {y.untyped_storage().data_ptr() == x.untyped_storage().data_ptr()}')\n",
    "print(f'\\'k\\' and \\'x\\' share same storage: {k.untyped_storage().data_ptr() == x.untyped_storage().data_ptr()}')\n",
    "print(f'\\'w\\' and \\'k\\' share same storage: {w.untyped_storage().data_ptr() == k.untyped_storage().data_ptr()}')\n",
    "# since k shares the same storage with x, so do w\n",
    "print(f'\\'w\\' and \\'x\\' share same storage: {w.untyped_storage().data_ptr() == x.untyped_storage().data_ptr()}\\n')\n",
    "\n",
    "# modofications of values in w will yield changes in x and k\n",
    "w[1,1] = 0.5\n",
    "print(x[1,1])\n",
    "print(k[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6733081f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6733081f",
    "outputId": "a4390288-be68-47bf-eca0-c1b2a54e97fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3790, 0.3809, 0.6042],\n",
      "        [0.6416, 0.5678, 0.3592]])\n",
      " stride: (3, 1)\n",
      " is contiguous: True\n",
      "\n",
      "tensor([[0.3790, 0.6416],\n",
      "        [0.3809, 0.5678]])\n",
      " stride: (1, 3)\n",
      " is contiguous: False\n",
      "\n",
      "tensor([[0.3790, 0.6416],\n",
      "        [0.3809, 0.5678]])\n",
      " stride: (2, 1)\n",
      " is contiguous: True\n",
      "\n",
      "'h' and 'x' share same storage: True\n",
      "'i' and 'h' share same storage: False\n"
     ]
    }
   ],
   "source": [
    "# Example 3: torch.as_strided(input, size, stride, storage_offset=None) might produce a non-contiguous view\n",
    "\n",
    "x = torch.rand(2, 3)\n",
    "print(x)\n",
    "print(f' stride: {x.stride()}\\n is contiguous: {x.is_contiguous()}\\n')\n",
    "\n",
    "h = torch.as_strided(x, (2,2), (1,3))\n",
    "print(h)\n",
    "print(f' stride: {h.stride()}\\n is contiguous: {h.is_contiguous()}\\n')\n",
    "\n",
    "# create a contiguous copy of h\n",
    "i = h.contiguous()\n",
    "print(i)\n",
    "print(f' stride: {i.stride()}\\n is contiguous: {i.is_contiguous()}\\n')\n",
    "\n",
    "# verify storage\n",
    "print(f'\\'h\\' and \\'x\\' share same storage: {h.untyped_storage().data_ptr() == x.untyped_storage().data_ptr()}')\n",
    "# tensor i is not a view of tensor h\n",
    "print(f'\\'i\\' and \\'h\\' share same storage: {h.untyped_storage().data_ptr() == i.untyped_storage().data_ptr()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iB7yb8HkUaAO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iB7yb8HkUaAO",
    "outputId": "c8e45da6-acee-402e-c0a8-3558784707f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 6.],\n",
      "        [2., 4.],\n",
      "        [5., 9.]])\n",
      " stride: (2, 1)\n",
      " is contiguous: True\n",
      "\n",
      "tensor([[6., 5.],\n",
      "        [2., 9.]])\n",
      " stride: (1, 3)\n",
      " is contiguous: False\n",
      "\n",
      "tensor([[6.],\n",
      "        [5.],\n",
      "        [2.],\n",
      "        [9.]])\n",
      " stride: (1, 1)\n",
      " is contiguous: True\n",
      "\n",
      "'y2' and 'x2' share same storage: True\n",
      "'z2' and 'y2' share same storage: False\n"
     ]
    }
   ],
   "source": [
    "# Special cases: torch.reshape(input, shape) sometimes returns a copy instead of a view\n",
    "\n",
    "x2 = torch.tensor([[3.0, 6.0], [2.0, 4.0], [5.0, 9.0]])\n",
    "print(x2)\n",
    "print(f' stride: {x2.stride()}\\n is contiguous: {x2.is_contiguous()}\\n')\n",
    "\n",
    "y2 = torch.as_strided(x2, (2,2), (1,3), 1)\n",
    "print(y2)\n",
    "print(f' stride: {y2.stride()}\\n is contiguous: {y2.is_contiguous()}\\n')\n",
    "\n",
    "z2 = torch.reshape(y2, (4,1))\n",
    "print(z2)\n",
    "# tensor z2 is a contiguous copy of y2\n",
    "print(f' stride: {z2.stride()}\\n is contiguous: {z2.is_contiguous()}\\n')\n",
    "\n",
    "# verify storage\n",
    "print(f'\\'y2\\' and \\'x2\\' share same storage: {y2.untyped_storage().data_ptr() == x2.untyped_storage().data_ptr()}')\n",
    "# reshape yields a copy instead of a view\n",
    "print(f'\\'z2\\' and \\'y2\\' share same storage: {z2.untyped_storage().data_ptr() == y2.untyped_storage().data_ptr()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0259d8c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0259d8c5",
    "outputId": "323aa2f0-1f29-4666-d6d6-16db2522cb89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Sum-up. Palying around with tensor contiguity\n",
    "\n",
    "X = torch.tensor([[2.0, 1.0], [4.0, 3.0], [6.0, 5.0]])\n",
    "print(X.is_contiguous()) #output: True\n",
    "\n",
    "Y = torch.reshape(X, (2,3))\n",
    "print(Y.is_contiguous()) #output: True\n",
    "\n",
    "Z = torch.as_strided(X, (2,2), (3,1), 1)\n",
    "#print(Z)\n",
    "print(Z.is_contiguous()) #output: False\n",
    "\n",
    "Z2 = torch.reshape(Z, (4,1))\n",
    "#print(Z2)\n",
    "print(Z2.is_contiguous()) #output: True\n",
    "\n",
    "# Y is a view of X\n",
    "print(Y.untyped_storage().data_ptr() == X.untyped_storage().data_ptr()) #output: True\n",
    "\n",
    "# Z is a view of X\n",
    "print(Z.untyped_storage().data_ptr() == X.untyped_storage().data_ptr()) #output: True\n",
    "\n",
    "# Z2 is a view of X\n",
    "print(Z2.untyped_storage().data_ptr() == X.untyped_storage().data_ptr()) #output: False\n",
    "\n",
    "D = Y.contiguous()\n",
    "# D is a view of Y\n",
    "print(D.untyped_storage().data_ptr() == X.untyped_storage().data_ptr()) #output: True\n",
    "\n",
    "G = Z.contiguous()\n",
    "# G is a copy of Z\n",
    "print(G.untyped_storage().data_ptr() == X.untyped_storage().data_ptr()) #output: False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22169788",
   "metadata": {
    "id": "22169788"
   },
   "source": [
    "### in-place methods (methods with trailing underscore)\n",
    "in-place methods allow the operations to modify the self tensor instead of returning a new tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17f178e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e17f178e",
    "outputId": "f169c73d-6c6d-49be-a519-e1bf98f0a8c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [1., 5.]])\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "B = torch.tensor([[2.0, 4.0], [1.0, 5.0]])\n",
    "print(B)\n",
    "#output:\n",
    "\n",
    "B.zero_()\n",
    "print(B)\n",
    "#output:\n",
    "\n",
    "B.requires_grad_(True)\n",
    "print(B)\n",
    "#output:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c134a6d2",
   "metadata": {
    "id": "c134a6d2"
   },
   "source": [
    "### The OUT argument\n",
    "Allows to provide a tensor as an argument to receive the output of the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d98cec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d0d98cec",
    "outputId": "0e063bf3-8b7e-4d39-b01c-bdda3eadb0f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "does z id matches original z_id? True\n",
      "\n",
      "is z id same as d id? True\n",
      "\n",
      "address of first element in d storage:\n",
      "116110208\n",
      "\n",
      "address of first element in z storage:\n",
      "116110208\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to create a new tensor on the storage of a existing one specify the argument out\n",
    "\n",
    "x = torch.rand(2,3)\n",
    "y = torch.ones(2,3)\n",
    "z = torch.zeros(2,3)\n",
    "\n",
    "# grab tensor's id ()\n",
    "z_id = id(z)\n",
    "\n",
    "# add x and y values and pass z as out argument\n",
    "d = torch.add(x, y, alpha = 10, out = z)\n",
    "\n",
    "# new z and old z are the same (same memory address)\n",
    "print(f'does z id matches original z_id? {id(z)==z_id}\\n')\n",
    "\n",
    "# new tensor d has the same memory address of z \n",
    "print(f'is z id same as d id? {id(z)==id(d)}\\n')\n",
    "\n",
    "# memory address of the first element of tensor\n",
    "print(f'address of first element in d storage:\\n{d.untyped_storage().data_ptr()}\\n')\n",
    "print(f'address of first element in z storage:\\n{z.untyped_storage().data_ptr()}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d58519",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19d58519",
    "outputId": "48855628-529d-4497-ab68-1a26f77fb54b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# further modify z values by passing it as out argument to a function\n",
    "torch.ones(2,3, out=z)\n",
    "print(z)\n",
    "\n",
    "print(id(z) == z_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd1a589",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afd1a589",
    "outputId": "67346e28-15ab-4e7e-d30c-49322d097036"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# another example:\n",
    "\n",
    "# define 3 tensors\n",
    "x = torch.empty(2,3)\n",
    "y = torch.arange(6).reshape(2,3)\n",
    "z = torch.full((2,3), fill_value = 2)\n",
    "\n",
    "# provide tensor x as out argument\n",
    "w = torch.add(y, z, alpha = 10, out = x)\n",
    "\n",
    "# compare Size and elements equality\n",
    "print(torch.equal(x, w))\n",
    "#output: True\n",
    "\n",
    "# compare addresses of first element in storage\n",
    "print(x.untyped_storage().data_ptr() == w.untyped_storage().data_ptr()) \n",
    "#output: True\n",
    "\n",
    "# compare tensor's memory addresses\n",
    "print(id(x) == id(w))\n",
    "#output: True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed945ae",
   "metadata": {
    "id": "5ed945ae"
   },
   "source": [
    "### Copying tensors\n",
    "Use `torch.clone()` method to copy tensor values into a new storage and preserve the memory format of the input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd236c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "acd236c4",
    "outputId": "aa61af92-0e06-4d5f-c2c7-c84692b9a889"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[0.2,0.7,0.3],[0.1,0.3,0.8]])\n",
    "\n",
    "#print(a)\n",
    "\n",
    "# an attribution operation returns a view of tensor\n",
    "# b is a view into a storage\n",
    "b = a\n",
    "\n",
    "# let's check the storage with id() and data_ptr() python and pytorch methods, repctivelly\n",
    "# a and b data share the same memory address\n",
    "print(id(a) == id(b))\n",
    "# a and b share the same storage\n",
    "print(a.untyped_storage().data_ptr() == b.untyped_storage().data_ptr())\n",
    "\n",
    "# use torch.clone() to create an independent copy (with its own storage) of a\n",
    "c = a.clone()\n",
    "# check if c and a share the same storage\n",
    "print(a.untyped_storage().data_ptr() == c.untyped_storage().data_ptr())\n",
    "\n",
    "#print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd0d740",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fcd0d740",
    "outputId": "37662f9f-affa-4cfd-91ff-454640397f30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2000, 0.1000],\n",
      "        [0.7000, 0.3000],\n",
      "        [0.3000, 0.8000]])\n",
      "False\n",
      "tensor([[0.2000, 0.1000],\n",
      "        [0.7000, 0.3000],\n",
      "        [0.3000, 0.8000]])\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# create a view of a using transpose() function\n",
    "d = a.transpose(0,1)\n",
    "print(d)\n",
    "\n",
    "# verify contiguity of d\n",
    "print(d.is_contiguous())\n",
    "\n",
    "# create a separate copy of d\n",
    "e = d.clone()\n",
    "print(e)\n",
    "\n",
    "# check if d (a view of a) and e (a clone of d) share the same storage\n",
    "print(d.untyped_storage().data_ptr() == e.untyped_storage().data_ptr())\n",
    "print(e.is_contiguous())\n",
    "\n",
    "# create a contiguos copy of e\n",
    "f = d.contiguous()\n",
    "#print(f)\n",
    "\n",
    "# check if d and f share the same storage\n",
    "print(d.untyped_storage().data_ptr() == f.untyped_storage().data_ptr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1122850c",
   "metadata": {
    "id": "1122850c"
   },
   "source": [
    "### Manipulating shapes\n",
    "It's possible to modify a tensor shape while still preserving its number of elements.\n",
    "\n",
    "Most used methods are: `torch.squeeze(input, dim=None)` and `torch.unsqueeze(input, dim)`; `torch.reshape(input, shape)`; and `torch.flatten(input, start_dim=0, end_dim=- 1)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d49bfcd",
   "metadata": {
    "id": "6d49bfcd"
   },
   "source": [
    "#### 1) `torch.squeeze(input, dim=None)` and `torch.unsqueeze(input, dim)`\n",
    "Use the methods `torch.squeeze(input, dim=None)` and `torch.unsqueeze(input, dim)` to remove or add a dimension of extent 1 to a tensor\n",
    "\n",
    "Note: these functions only apply for dimensions of extent 1\n",
    "\n",
    "`dim`: int or tuple of indices representing the specified dimensions the input will be squeezed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f29e32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "07f29e32",
    "outputId": "72790bc7-d0e7-4f6e-9f83-d14fc41a08b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([1, 3, 4])\n",
      "False\n",
      "torch.Size([3, 4])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(12).reshape(3,4)\n",
    "print(a.size())\n",
    "\n",
    "b = a.unsqueeze(dim = 0)\n",
    "print(b.size())\n",
    "# compare Size and elements equality\n",
    "print(torch.equal(a, b))\n",
    "\n",
    "c = b.squeeze(dim = 0)\n",
    "print(c.size())\n",
    "# compare Size and elements equality\n",
    "print(torch.equal(b, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4f455f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "df4f455f",
    "outputId": "da6ea99e-e226-4e73-9574-d38f4c1d31cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 2, 1, 2])\n",
      "torch.Size([2, 2, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(2, 1, 2, 1, 2)\n",
    "print(x.size())\n",
    "\n",
    "y = torch.squeeze(x, 1)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5af7775",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5af7775",
    "outputId": "32bdf1fe-4ba7-4385-de9b-449799281b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 3, 4]])\n",
      "tensor([[2],\n",
      "        [3],\n",
      "        [4]])\n"
     ]
    }
   ],
   "source": [
    "# torch.unsqueeze(input, dim) returns a new tensor with a dimension of size one inserted at the specified position.\n",
    "\n",
    "x = torch.tensor([2, 3, 4])\n",
    "print(torch.unsqueeze(x, 0))\n",
    "\n",
    "print(torch.unsqueeze(x, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3004e91e",
   "metadata": {
    "id": "3004e91e"
   },
   "source": [
    "#### 2) `torch.reshape(input, shape)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5930719",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5930719",
    "outputId": "9a134c1f-1163-41ba-bca7-f6f5aaf54032",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3385, 0.2512, 0.2215, 0.2505, 0.3464],\n",
      "         [0.2060, 0.0754, 0.8791, 0.3030, 0.4763],\n",
      "         [0.8611, 0.3436, 0.1828, 0.0882, 0.6567]],\n",
      "\n",
      "        [[0.9485, 0.5098, 0.3869, 0.3874, 0.7220],\n",
      "         [0.6165, 0.2554, 0.8959, 0.7414, 0.1511],\n",
      "         [0.9348, 0.9783, 0.9442, 0.4174, 0.9970]]])\n",
      "torch.Size([2, 3, 5])\n",
      "\n",
      "\n",
      "tensor([0.3385, 0.2512, 0.2215, 0.2505, 0.3464, 0.2060, 0.0754, 0.8791, 0.3030,\n",
      "        0.4763, 0.8611, 0.3436, 0.1828, 0.0882, 0.6567, 0.9485, 0.5098, 0.3869,\n",
      "        0.3874, 0.7220, 0.6165, 0.2554, 0.8959, 0.7414, 0.1511, 0.9348, 0.9783,\n",
      "        0.9442, 0.4174, 0.9970])\n",
      "torch.Size([30])\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "\n",
    "a = torch.rand(2, 3, 5)\n",
    "print(a)\n",
    "print(a.size())\n",
    "\n",
    "print('\\n')\n",
    "b = a.reshape(2*3*5)\n",
    "print(b)\n",
    "print(b.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9045fc",
   "metadata": {
    "id": "4f9045fc"
   },
   "source": [
    "#### 3) `torch.flatten(input, start_dim=0, end_dim=- 1)`\n",
    "The method `torch.flatten(input, start_dim=0, end_dim=- 1)` flattens input by reshaping it into a one-dimensional tensor. This method may return the original object, a view, or copy. (learn more at: https://pytorch.org/docs/stable/generated/torch.flatten.html?highlight=torch+flatten#torch.flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeb2e99",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "adeb2e99",
    "outputId": "011856c8-c3cf-4222-9af6-f11fc6fe92b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(100).reshape(2, 5, 10)\n",
    "\n",
    "x = torch.flatten(x)\n",
    "print(x.shape)\n",
    "\n",
    "x = torch.flatten(x, start_dim=-1)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe858a7c",
   "metadata": {
    "id": "fe858a7c"
   },
   "source": [
    "### Splitting tensors\n",
    "Split tensors using the method `torch.split(tensor, split_size_or_sections, dim=0)`, where `split_size_or_sections` is an integer or a list of integers representing either the size of a single chunk or a list of sizes for each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d18eb6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5d18eb6",
    "outputId": "052468a7-23cd-4a95-8d2a-d19f616a89b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5],\n",
      "        [6, 7],\n",
      "        [8, 9]])\n",
      "\n",
      "\n",
      "(tensor([[0, 1],\n",
      "        [2, 3]]), tensor([[4, 5],\n",
      "        [6, 7]]), tensor([[8, 9]]))\n",
      "\n",
      "\n",
      "(tensor([[0, 1]]), tensor([[2, 3],\n",
      "        [4, 5],\n",
      "        [6, 7],\n",
      "        [8, 9]]))\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(10).reshape(5, 2)\n",
    "print(a)\n",
    "print('\\n')\n",
    "\n",
    "# split into chunks of 2 rows each\n",
    "print(torch.split(a, 2))\n",
    "print('\\n')\n",
    "\n",
    "# split into 2 chunks, one with 1 row and the other with 4 rows\n",
    "print(torch.split(a, [1, 4]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b50da92",
   "metadata": {
    "id": "3b50da92"
   },
   "source": [
    "### Mathematical operations with tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4154050",
   "metadata": {
    "id": "b4154050"
   },
   "source": [
    "#### 1) Element-wise operations: addition, subtraction, multiplication, division, power of, exponential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2808e73",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b2808e73",
    "outputId": "b10fa4b8-f0d8-4c73-d588-34e230cf185b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Scalar s:\n",
      "tensor([0.4255])\n",
      " Tensor A:\n",
      "tensor([[ 0.1200,  0.1500, -0.0100],\n",
      "        [-1.1300, -2.0400, -1.3800]])\n",
      " Vector B:\n",
      "tensor([0.8700, 0.4200, 0.8500, 0.9800, 0.3100, 0.1400])\n",
      "\n",
      "\n",
      " s + A:\n",
      "tensor([[ 0.5455,  0.5755,  0.4155],\n",
      "        [-0.7045, -1.6145, -0.9545]])\n",
      " s + v:\n",
      "tensor([1.2955, 0.8455, 1.2755, 1.4055, 0.7355, 0.5655])\n",
      " A - s:\n",
      "tensor([[-0.3055, -0.2755, -0.4355],\n",
      "        [-1.5555, -2.4655, -1.8055]])\n",
      " v - s:\n",
      "tensor([ 0.4445, -0.0055,  0.4245,  0.5545, -0.1155, -0.2855])\n"
     ]
    }
   ],
   "source": [
    "# add or subtract a scalar to a vector or to a matrix with element-wise operation\n",
    "\n",
    "s = torch.rand(1)\n",
    "A = torch.tensor([[ 0.12,  0.15, -0.01], [-1.13, -2.04, -1.38]])\n",
    "v = torch.tensor([0.87, 0.42, 0.85, 0.98, 0.31, 0.14])\n",
    "\n",
    "print(f' Scalar s:\\n{s}')\n",
    "print(f' Tensor A:\\n{A}')\n",
    "print(f' Vector B:\\n{v}')\n",
    "print('\\n')\n",
    "\n",
    "# add a scalar to a vector or matrix\n",
    "print(f' s + A:\\n{torch.add(s, A)}')\n",
    "print(f' s + v:\\n{torch.add(s, v)}')\n",
    "\n",
    "# subtract a scalar to a vector or matrix\n",
    "print(f' A - s:\\n{torch.subtract(A, s)}')\n",
    "print(f' v - s:\\n{torch.subtract(v, s)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13cc7bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f13cc7bb",
    "outputId": "e47307f5-3334-4ebe-a973-ed43596bc79f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor B:\n",
      "tensor([[2., 1., 4.],\n",
      "        [3., 7., 5.]])\n",
      "tensor C:\n",
      "tensor([[ 5.0000, 10.0000,  2.5000],\n",
      "        [ 3.3000,  1.5000,  2.0000]])\n",
      "Addition (B+C):\n",
      " tensor([[ 7.0000, 11.0000,  6.5000],\n",
      "        [ 6.3000,  8.5000,  7.0000]])\n",
      "Subtraction (B-C):\n",
      " tensor([[-3.0000, -9.0000,  1.5000],\n",
      "        [-0.3000,  5.5000,  3.0000]])\n",
      "Multiplication (BxC):\n",
      " tensor([[10.0000, 10.0000, 10.0000],\n",
      "        [ 9.9000, 10.5000, 10.0000]])\n",
      "Division (B/C):\n",
      " tensor([[0.4000, 0.1000, 1.6000],\n",
      "        [0.9091, 4.6667, 2.5000]])\n",
      "Rise to the power of (B^C):\n",
      ": tensor([[32.0000,  1.0000, 32.0000],\n",
      "        [37.5405, 18.5203, 25.0000]])\n"
     ]
    }
   ],
   "source": [
    "# Element-wise operations over multidimensional input tensors\n",
    "# addition, subtraction, multiplication, division, power of\n",
    "\n",
    "B = torch.tensor([[2.0, 1.0, 4.0], [3.0, 7.0, 5.0]])\n",
    "C = torch.tensor([[5.0, 10.0, 2.5], [3.3, 1.5, 2.0]])\n",
    "print(f'tensor B:\\n{B}')\n",
    "print(f'tensor C:\\n{C}')\n",
    "\n",
    "# add arguments element-wise. Equivalent to b+c\n",
    "print(f'Addition (B+C):\\n {torch.add(B, C)}')\n",
    "# subtracts arguments element-wise. Equivalent to b-c in broadcasting\n",
    "print(f'Subtraction (B-C):\\n {torch.subtract(B, C)}')\n",
    "# multiply each element of the first tensor by corresponding element in second tensor. Equivalent to b*c in terms of broadcasting\n",
    "print(f'Multiplication (BxC):\\n {torch.multiply(B, C)}')\n",
    "# divide each element of the first tensor by corresponding element in second tensor. Same as b/c in terms of broadcasting\n",
    "print(f'Division (B/C):\\n {torch.divide(B, C)}')\n",
    "# rises each element of the first tensor to the power of corresponding element in second tensor. Equivalent to b**c\n",
    "print(f'Rise to the power of (B^C):\\n: {torch.pow(B, C)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qAjyST-DeGdr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qAjyST-DeGdr",
    "outputId": "cb881728-d4a2-46b0-d71b-be2684c0eb33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiplication (BxC) with torch.multiply():\n",
      " tensor([[10.0000, 10.0000, 10.0000],\n",
      "        [ 9.9000, 10.5000, 10.0000]])\n",
      "Multiplication (BxC) with torch.mul():\n",
      " tensor([[10.0000, 10.0000, 10.0000],\n",
      "        [ 9.9000, 10.5000, 10.0000]])\n",
      "Multiplication (BxC) with * operator:\n",
      " tensor([[10.0000, 10.0000, 10.0000],\n",
      "        [ 9.9000, 10.5000, 10.0000]])\n"
     ]
    }
   ],
   "source": [
    "# element-wise multiplication\n",
    "\n",
    "print(f'Multiplication (BxC) with torch.multiply():\\n {torch.multiply(B, C)}')\n",
    "print(f'Multiplication (BxC) with torch.mul():\\n {torch.mul(B, C)}')\n",
    "print(f'Multiplication (BxC) with * operator:\\n {B * C}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc06ee54",
   "metadata": {
    "id": "cc06ee54"
   },
   "source": [
    "#### 2) Matrix operations (Linear Algebra): transpose, determinant, inverse, eye, matrix multiplication.\n",
    "\n",
    "1) `torch.linalg` methods:\n",
    "\n",
    "* `torch.linalg.det(A, *, out=None)`\n",
    "* `torch.linalg.inv(A, *, out=None)`\n",
    "* `torch.linalg.matmul(input, other, *, out=None)` -- equivalent to `@` operator\n",
    "\n",
    "To earn more about `torch.linalg` see: https://pytorch.org/docs/stable/linalg.html\n",
    "\n",
    "2) Other methods:\n",
    "\n",
    "* `torch.t(input)`\n",
    "* `torch.transpose(input, dim0, dim1)`\n",
    "* `torch.eye(n, m=n)`\n",
    "* `torch.Tensor.mm()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a231fc96",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a231fc96",
    "outputId": "6d9a5e84-c0b0-4dbe-a20b-c1b07129671c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2669, 0.6401, 0.0550],\n",
      "        [0.0902, 0.9186, 0.5040],\n",
      "        [0.5403, 0.8227, 0.1301]])\n",
      "\n",
      "\n",
      "tensor([[0.2669, 0.0902, 0.5403],\n",
      "        [0.6401, 0.9186, 0.8227],\n",
      "        [0.0550, 0.5040, 0.1301]])\n",
      "tensor([[0.2669, 0.0902, 0.5403],\n",
      "        [0.6401, 0.9186, 0.8227],\n",
      "        [0.0550, 0.5040, 0.1301]])\n",
      "\n",
      "\n",
      "tensor(0.0648)\n",
      "tensor(0.0648)\n",
      "\n",
      "\n",
      "tensor([[-4.5535, -0.5873,  4.1986],\n",
      "        [ 4.0204,  0.0776, -1.9992],\n",
      "        [-6.5130,  1.9478,  2.8925]])\n",
      "\n",
      "\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n",
      "\n",
      "\n",
      "tensor([[0.2669, 0.6401, 0.0550],\n",
      "        [0.0902, 0.9186, 0.5040],\n",
      "        [0.5403, 0.8227, 0.1301]])\n",
      "\n",
      "\n",
      "tensor([[ 1.0000e+00, -7.4506e-09,  8.9407e-08],\n",
      "        [ 2.3842e-07,  1.0000e+00,  0.0000e+00],\n",
      "        [-5.9605e-08, -2.9802e-08,  1.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "# create matrix A\n",
    "A = torch.rand(3,3)\n",
    "print(A)\n",
    "print('\\n')\n",
    "\n",
    "# compute A transpose\n",
    "print(A.t())\n",
    "print(torch.transpose(A, 0, 1))\n",
    "print('\\n')\n",
    "\n",
    "# compute determinat of A\n",
    "print(A.det())\n",
    "print(torch.linalg.det(A))\n",
    "print('\\n')\n",
    "\n",
    "# compute inverse of A\n",
    "print(torch.linalg.inv(A))\n",
    "print('\\n')\n",
    "\n",
    "# compute the eye of A\n",
    "print(torch.eye(3,3))\n",
    "print('\\n')\n",
    "\n",
    "# compute matrix multiplication (matrices shapes must be compatible)\n",
    "print(torch.linalg.matmul(A, torch.eye(3,3)))\n",
    "print('\\n')\n",
    "print(torch.linalg.matmul(A, torch.linalg.inv(A)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92de415",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f92de415",
    "outputId": "2f4137e3-497d-458b-8a29-843d58bf6258"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12.7122, 16.1172],\n",
      "        [19.9703, 24.5590]])\n",
      "tensor([[19.0684, 22.4733, 25.8783],\n",
      "        [29.9555, 34.5442, 39.1329]])\n"
     ]
    }
   ],
   "source": [
    "# the method torch.Tensor.mm() --> matrix multiplication: linear combination of vectors\n",
    "# matrices shapes must be compatible\n",
    "\n",
    "X = torch.rand(2,6)\n",
    "Y = torch.arange(12, dtype = torch.float32).reshape((6,2))\n",
    "Z = torch.arange(18, dtype = torch.float32).reshape((6,3))\n",
    "print(torch.mm(X, Y))\n",
    "print(torch.mm(X, Z))\n",
    "#print(torch.mm(Y, Z)) # ERROR: mat1 and mat2 shapes cannot be multiplied (6x2 and 6x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea3f99a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aea3f99a",
    "outputId": "08e0ef9d-32a0-4f23-bf76-a9a430861a83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x size: torch.Size([2, 3]), y size: torch.Size([3, 1])\n",
      "tensor([[ 9.],\n",
      "        [17.]])\n"
     ]
    }
   ],
   "source": [
    "# the @ operator --> matrix multiplication: linear combination of vectors\n",
    "# matrices shapes must be compatible (A(ixn) * B(nxj) = X(ixj)\n",
    "\n",
    "x = torch.tensor([[2.0, 1.0, 4.0], [3.0, 7.0, 5.0]])\n",
    "y = torch.arange(3, dtype = torch.float32).unsqueeze(dim = 1)\n",
    "\n",
    "print(f'x size: {x.size()}, y size: {y.size()}')\n",
    "\n",
    "print(x @ y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac8526",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "feac8526",
    "outputId": "b3a68b33-74be-4dd5-9908-23d2bd24bb28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5113, 0.4985, 0.0753],\n",
      "        [0.2521, 0.5372, 0.5058]])\n",
      "tensor([[0., 1.],\n",
      "        [2., 3.],\n",
      "        [4., 5.]])\n",
      "\n",
      "\n",
      "tensor([[1.2983, 2.3833],\n",
      "        [3.0976, 4.3927]])\n",
      "\n",
      "\n",
      "tensor([[1.2983, 2.3833],\n",
      "        [3.0976, 4.3927]])\n",
      "\n",
      "\n",
      "tensor([[1.2983, 2.3833],\n",
      "        [3.0976, 4.3927]])\n"
     ]
    }
   ],
   "source": [
    "# matrix multiplications with linear combination of vectors -- shapes must be compatible\n",
    "\n",
    "D = torch.rand(2,3)\n",
    "E = torch.arange(6, dtype = torch.float32).reshape((3,2))\n",
    "print(D)\n",
    "print(E)\n",
    "print('\\n')\n",
    "\n",
    "# using torch.linalg.matmul()\n",
    "print(torch.linalg.matmul(D, E))\n",
    "print('\\n')\n",
    "# using torch.mm()\n",
    "print(torch.mm(D, E))\n",
    "print('\\n')\n",
    "# using @\n",
    "print(D @ E) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bc8abf",
   "metadata": {
    "id": "c7bc8abf"
   },
   "source": [
    "#### 3) Dot product (or inner product): is the sum of the products of each corresponding element\n",
    "\n",
    "Use the method `torch.dot(input, other, out=None)` to perform dot product between two 1D tensors with the same number of elements.\n",
    "\n",
    "Use the method `torch.inner(input, other, out=None)` to perform dot product between tensors of higher dimensions. This method sums the product of elements from input and other along their last dimension. Note: If both input and other are non-scalars, the size of their last dimension must match.\n",
    "\n",
    "Use the method `torch.mm(input, other, out=None)` to perform dot product between 2D tensors. This method sums the product of elements from input and other along their last dimension. Note: apply only to matrices of the same size and shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc021de",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3fc021de",
    "outputId": "8bf076a1-6eef-49ea-a207-42db987ea0f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "# torch.dot(input, other, out=None) -- applies to 1D tensors with the same number of elements\n",
    "\n",
    "# 1D tensors: Dot product\n",
    "print(torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1])))\n",
    "\n",
    "# Scalar input\n",
    "#print(torch.dot(torch.tensor([4, 5, 6]), torch.tensor(2))) # ERROR: 1D tensors expected, but got 1D and 0D tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66043335",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66043335",
    "outputId": "6991e543-8a7d-412b-c118-4bcce9dab5c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.5392,  1.2391,  2.6282],\n",
      "        [ 1.4203, -1.5125,  3.5483]])\n",
      "\n",
      "\n",
      "tensor(7)\n",
      "\n",
      "\n",
      "tensor([[ 1.4918,  0.1814, -1.6719],\n",
      "        [ 2.0150, -0.7804, -0.2495]])\n",
      "tensor([[[ 0.8480,  2.1880, -0.7319],\n",
      "         [ 1.0656, -0.4168,  0.0910],\n",
      "         [-1.1119,  1.2358,  0.8453],\n",
      "         [-0.6829,  0.9956, -0.1030]],\n",
      "\n",
      "        [[-0.9585,  0.3884, -0.1473],\n",
      "         [-0.3188, -2.3169, -1.2417],\n",
      "         [ 0.4717, -0.7689,  1.4844],\n",
      "         [ 0.6470, -0.2971, -0.0756]]])\n",
      "tensor([[[ 2.8857,  1.3619, -2.8477, -0.6658],\n",
      "         [-1.1130,  1.1801, -1.9177,  1.0377]],\n",
      "\n",
      "        [[ 0.1837,  2.4498, -3.4158, -2.1273],\n",
      "         [-2.1977,  1.4756,  1.1802,  1.5545]]])\n"
     ]
    }
   ],
   "source": [
    "# torch.inner(input, other, out=None) -- applies to 1D and to higher dimension tensors\n",
    "# do work for scalar x tensor multiplication\n",
    "\n",
    "# Scalar input\n",
    "print(torch.inner(torch.tensor(2), torch.randn(2, 3)))\n",
    "print('\\n')\n",
    "\n",
    "# 1D input tensors\n",
    "# Note: The size of the last dimension must match.\n",
    "print(torch.inner(torch.tensor([1, 2, 3]), torch.tensor([0, 2, 1])))\n",
    "print('\\n')\n",
    "\n",
    "# Multidimensional input tensors: perform dot product along the tensors last dimension.\n",
    "# Note: The size of the last dimension must match.\n",
    "a = torch.randn(2, 3)\n",
    "print(a)\n",
    "\n",
    "b = torch.randn(2, 4, 3)\n",
    "print(b)\n",
    "\n",
    "print(torch.inner(a, b))\n",
    "\n",
    "# the result is equivalent to torch.tensordot(input, other, dims=([-1], [-1]))\n",
    "#print(torch.tensordot(a, b, dims=([-1], [-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51fd682",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b51fd682",
    "outputId": "4b1d7576-a0c6-4034-be75-6cb1e2470a6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n",
      "tensor(7)\n",
      "tensor(7)\n",
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "# Dot product for 1D tensors (do not apply with torch.mm())\n",
    "print(torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1])))\n",
    "print(torch.inner(torch.tensor([2, 3]), torch.tensor([2, 1])))\n",
    "print(torch.matmul(torch.tensor([2, 3]), torch.tensor([2, 1])))\n",
    "print(torch.tensor([2, 3]) @ torch.tensor([2, 1]))\n",
    "\n",
    "#print(torch.mm(torch.tensor([2, 3]), torch.tensor([2, 1]))) ### ERROR: self must be a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mRBGjO2eukK1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mRBGjO2eukK1",
    "outputId": "3fa9a8e0-76e0-45d0-b18e-10f27e55d669"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2265,  0.9129,  0.7441],\n",
      "        [ 1.1507, -2.5597,  0.6432]])\n",
      "tensor([[[ 0.0679,  1.2022, -2.2437],\n",
      "         [ 0.6421, -0.0036, -0.5857],\n",
      "         [-0.8509,  0.1924,  0.7389],\n",
      "         [-0.3911, -0.2389,  1.2887]],\n",
      "\n",
      "        [[-2.2724,  0.3461, -0.1914],\n",
      "         [-0.9517,  0.2594,  2.8443],\n",
      "         [-0.9623, -0.0471,  0.5775],\n",
      "         [-0.6221,  0.3150, -0.1248]]])\n",
      "\n",
      " inner operator\n",
      "tensor([[[-0.4888,  0.3484, -0.3182,  0.2611],\n",
      "         [-2.6135,  1.1860, -0.7935, -0.5683]],\n",
      "\n",
      "        [[-4.4424,  0.3714, -0.9965,  0.9902],\n",
      "         [-3.6241,  0.0702, -0.6152, -1.6025]]])\n"
     ]
    }
   ],
   "source": [
    "# Dot product / matrix multiplication for multidimensional inputs with different shapes\n",
    "# different shapes => torch.inner()\n",
    "\n",
    "a = torch.randn(2, 3)\n",
    "print(a)\n",
    "\n",
    "b = torch.randn(2, 4, 3)\n",
    "print(b)\n",
    "\n",
    "print(f'\\n inner operator')\n",
    "print(torch.inner(a, b))\n",
    "\n",
    "#print(f'\\n mm operator')\n",
    "#print(torch.mm(a, b)) # Error: mat2 must be a matrix\n",
    "\n",
    "#print(f'\\n matmul operator')\n",
    "#print(torch.matmul(a, b)) # Error: mat1 and mat2 shapes cannot be multiplied (6x4 and 3x2)\n",
    "\n",
    "#print(f'\\n @ operator')\n",
    "#print(a @ b) # Error: mat1 and mat2 shapes cannot be multiplied (6x4 and 3x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ToRZhl0QxSMI",
   "metadata": {
    "id": "ToRZhl0QxSMI"
   },
   "source": [
    "#### - matrix multiplication summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i48bcTOXgrRj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i48bcTOXgrRj",
    "outputId": "c14f5164-30d9-4482-daf9-f5cc8c88412f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element-wise multiplication:\n",
      " tensor([[10.0000, 10.0000, 10.0000],\n",
      "        [ 9.9000, 10.5000, 10.0000]])\n",
      "\n",
      "matrix multiplication:\n",
      " tensor([[1.5934],\n",
      "        [1.0471]])\n",
      "\n",
      "dot product:\n",
      " tensor([[[ 0.1091, -1.0031,  0.0366,  2.3827],\n",
      "         [ 1.7492, -0.6717, -0.6313,  2.3689]],\n",
      "\n",
      "        [[ 0.0415, -1.2001, -0.3059,  0.8838],\n",
      "         [ 2.5060, -0.8551, -2.1795,  1.0900]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# element-wise multiplication: The tensors last dimension size must match\n",
    "B = torch.tensor([[2.0, 1.0, 4.0], [3.0, 7.0, 5.0]])\n",
    "C = torch.tensor([[5.0, 10.0, 2.5], [3.3, 1.5, 2.0]])\n",
    "print(f'element-wise multiplication:\\n {torch.multiply(B, C)}\\n')\n",
    "\n",
    "# matrix multiplication through linear combination of vectors (row by column)\n",
    "D = torch.rand(2,3)\n",
    "E = torch.arange(3, dtype = torch.float32).reshape((3,1))\n",
    "print(f'matrix multiplication:\\n {torch.linalg.matmul(D, E)}\\n')\n",
    "\n",
    "# dot product: perform dot product along the tensors last dimension.\n",
    "# Note: The size of the last dimension must match.\n",
    "G = torch.randn(2, 3)\n",
    "J = torch.randn(2, 4, 3)\n",
    "print(f'dot product:\\n {torch.inner(G, J)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37922b7",
   "metadata": {
    "id": "c37922b7"
   },
   "source": [
    "### Broadcasting\n",
    "way of doing operations on tensors with shapes considered uncompatible by linear algebra\n",
    "\n",
    "Rules:\n",
    "- each tensor must have at least 1 dimension\n",
    "- comparing the tensors dimensions from right to left, or from the last to the first, their size must either:\n",
    "- 1) be equal, or\n",
    "- 2) one of the dimensions must be of size 1, or\n",
    "- 3) one of the dimensions should be absent in one of the tensors\n",
    "\n",
    "to learn more on broadcasting semantics and backwards compatibility see https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fhxHKTnOmGsd",
   "metadata": {
    "id": "fhxHKTnOmGsd"
   },
   "outputs": [],
   "source": [
    "x=torch.empty((0,))\n",
    "y=torch.empty(2,2)\n",
    "# x and y are not broadcastable, because x does not have at least 1 dimension\n",
    "\n",
    "#print(x * y) # ERROR: The size of tensor a (0) must match the size of tensor b (2) at non-singleton dimension 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QKtwZ2Z0pKOo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QKtwZ2Z0pKOo",
    "outputId": "cf7bd2f2-02dc-42a8-e7df-4eaf97aa9415"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., -0.],\n",
      "         [0., -0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., -0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., nan],\n",
      "         [-0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [-0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., -0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., -0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.empty(5,7,3)\n",
    "y=torch.empty(5,7,3)\n",
    "# same shapes are always broadcastable (i.e. the above rules always hold)\n",
    "print(x * y)\n",
    "\n",
    "x=torch.empty((0,))\n",
    "y=torch.empty(2,2)\n",
    "# x and y are not broadcastable, because x does not have at least 1 dimension\n",
    "#print(x * y) # ERROR: The size of tensor a (0) must match the size of tensor b (2) at non-singleton dimension 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bace1214",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bace1214",
    "outputId": "3959f858-000c-40a3-c2c4-e1df6686666c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5338, 1.0168, 1.0718, 0.7299],\n",
      "        [0.0678, 1.7494, 1.1978, 1.7939]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2,4)\n",
    "y = torch.ones(1,4)\n",
    "# x and y are broadcastable.\n",
    "# 1st trailing dimension: x size == y size\n",
    "# 2nd trailing dimension: y has size 1\n",
    "# 3rd trailing dimension: x size == y size\n",
    "print(x * (y * 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2e8c34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4a2e8c34",
    "outputId": "ae161bb9-500c-4327-8e90-7cd273a81988"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-2.9360e-01],\n",
      "          [ 3.9480e-01],\n",
      "          [-1.9569e-01],\n",
      "          [-4.7564e-02]],\n",
      "\n",
      "         [[ 6.2501e-02],\n",
      "          [-7.5487e-01],\n",
      "          [ 1.2323e-01],\n",
      "          [ 7.3563e-01]],\n",
      "\n",
      "         [[ 6.3212e-03],\n",
      "          [ 1.2984e-02],\n",
      "          [ 1.7448e-02],\n",
      "          [-1.5850e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.7100e-01],\n",
      "          [ 6.0374e-01],\n",
      "          [ 4.3250e-01],\n",
      "          [-7.8030e-01]],\n",
      "\n",
      "         [[ 1.2481e+00],\n",
      "          [ 2.1190e-01],\n",
      "          [-5.4301e-02],\n",
      "          [ 4.8851e-02]],\n",
      "\n",
      "         [[ 1.6022e-02],\n",
      "          [-1.6117e-03],\n",
      "          [-3.9112e-04],\n",
      "          [-1.2361e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.8876e-01],\n",
      "          [-5.9112e-01],\n",
      "          [-7.1652e-02],\n",
      "          [ 5.3304e-01]],\n",
      "\n",
      "         [[-3.4825e-01],\n",
      "          [-1.7068e-01],\n",
      "          [-1.0657e+00],\n",
      "          [-7.0671e-01]],\n",
      "\n",
      "         [[-6.9044e-03],\n",
      "          [-2.7209e-02],\n",
      "          [ 1.9179e-03],\n",
      "          [ 5.6053e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 8.3304e-01],\n",
      "          [-4.3798e-01],\n",
      "          [ 8.8767e-02],\n",
      "          [-5.8169e-01]],\n",
      "\n",
      "         [[ 9.9927e-01],\n",
      "          [-1.8424e+00],\n",
      "          [ 1.1765e+00],\n",
      "          [-2.1208e+00]],\n",
      "\n",
      "         [[-2.6181e-02],\n",
      "          [ 2.2462e-02],\n",
      "          [-1.8626e-02],\n",
      "          [-5.9695e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 3.4157e-01],\n",
      "          [ 3.6762e-01],\n",
      "          [-3.2130e-01],\n",
      "          [-2.4077e-01]],\n",
      "\n",
      "         [[-7.6317e-01],\n",
      "          [-4.7827e-01],\n",
      "          [-2.7420e-01],\n",
      "          [ 7.8508e-01]],\n",
      "\n",
      "         [[-3.9680e-03],\n",
      "          [-2.9970e-02],\n",
      "          [ 1.1649e-02],\n",
      "          [ 1.6904e-02]]]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(5,3,4,1)\n",
    "y=torch.randn(  3,1,1)\n",
    "# x and y are broadcastable.\n",
    "# 1st trailing dimension: both have size 1\n",
    "# 2nd trailing dimension: y has size 1\n",
    "# 3rd trailing dimension: x size == y size\n",
    "# 4th trailing dimension: y dimension doesn't exist\n",
    "print(x * y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70b7730",
   "metadata": {
    "id": "b70b7730"
   },
   "source": [
    "### PyTorch.torch.autograd package provide methods to compute automatic differentiation\n",
    "\n",
    "Use the method `torch.tensor.backward(gradient=None, inputs=None)` to compute the sum of gradients of input tensors with respect to graph leaves.\n",
    "\n",
    "Use the method `torch.tensor.detach()` to return a new tensor detached from the current graph.\n",
    "\n",
    "Use the method `torch.tensor.item()` to return the value (as a Python number) stored in the tensor (apply only to tensor with a sigle value).\n",
    "\n",
    "To learn more about `PyTorch.torch.autograd` package refer to: http://pytorch.org/docs/stable/autograd.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "uHmVKDFppAq0",
   "metadata": {
    "id": "uHmVKDFppAq0"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09e0aad4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09e0aad4",
    "outputId": "b8e9d772-5e28-4ab7-afd1-a4d501d97b12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor A:\n",
      "tensor([[11]])\n",
      "11\n",
      "tensor B:\n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor([[11]])\n",
    "print(f'tensor A:\\n{A}')\n",
    "print(A.item())\n",
    "\n",
    "B = torch.arange(1, 10)\n",
    "print(f'tensor B:\\n{B}')\n",
    "print(B.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "Y8dT-yqZyHVp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y8dT-yqZyHVp",
    "outputId": "28b3984e-db40-42bf-bd0e-34b43be35999"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- P tensor showing requires_grad:\n",
      "tensor([1., 2., 3.], requires_grad=True)\n",
      "\n",
      "\n",
      "- Y tensor:\n",
      "tensor([[ 0.6607,  2.7439, -1.7320],\n",
      "        [ 0.3982, -0.5768, -5.8050],\n",
      "        [-1.4563,  0.2384,  0.8608]], grad_fn=<MulBackward0>)\n",
      "- Y requires_grad:\n",
      "True\n",
      "- Y grad_fn:\n",
      "<MulBackward0 object at 0x7f737c77fe20>\n",
      "\n",
      "\n",
      "- L tensor:\n",
      "-0.5186777710914612\n",
      "- L requires_grad:\n",
      "True\n",
      "\n",
      "\n",
      "- gradient of P BEFORE backpropagation:\n",
      "None\n",
      "- gradient of P AFTER backpropagation:\n",
      "tensor([-0.0442,  0.1336, -0.2473])\n",
      "\n",
      "\n",
      "L requires_grad after computation whithin torch.no_grad():\n",
      "False\n",
      "\n",
      "\n",
      "requires_grad of L2\n",
      "True\n",
      "requires_grad of L2.detach()\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# create two tensors X (variables) and P (parameters)\n",
    "X = torch.randn(3,3)\n",
    "P = torch.arange(1, 4, dtype=torch.float32)\n",
    "\n",
    "# sets tensor's P requires_grad to True to enable automatic computation of gradients during backward pass\n",
    "P.requires_grad_(True)\n",
    "print(f'- P tensor showing requires_grad:\\n{P}')\n",
    "print('\\n')\n",
    "\n",
    "# perform an operation with tensors X and P (i.e. do a forward pass)\n",
    "Y = torch.multiply(X, P)\n",
    "print(f'- Y tensor:\\n{Y}')\n",
    "# note that Y requires_grad is also True\n",
    "print(f'- Y requires_grad:\\n{Y.requires_grad}')\n",
    "# Note: Y tensor has a property grad_fn to store a reference to the backpropagation fucntion\n",
    "print(f'- Y grad_fn:\\n{Y.grad_fn}')\n",
    "print('\\n')\n",
    "\n",
    "# compute a second operation, now over previous output, i.e. tensor Y\n",
    "L = Y.mean()\n",
    "print(f'- L tensor:\\n{L}')\n",
    "# note that L requires_grad is also True\n",
    "print(f'- L requires_grad:\\n{L.requires_grad}')\n",
    "print('\\n')\n",
    "\n",
    "# print X grad property before doing backward pass (grad = computed derivatives)\n",
    "print(f'- gradient of P BEFORE backpropagation:\\n{P.grad}')\n",
    "\n",
    "# compute backpropagation\n",
    "L.backward()\n",
    "\n",
    "# print X grad property after backward pass (grad = computed derivatives)\n",
    "print(f'- gradient of P AFTER backpropagation:\\n{P.grad}')\n",
    "print('\\n')\n",
    "\n",
    "### print(f'try to print the grad of a non-lef tensor:\\n{L.grad}')\n",
    "#Warning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed.\n",
    "\n",
    "# In order to NOT track the next operation in autograd we perform it whithin torch.no_grad()\n",
    "with torch.no_grad():\n",
    "  L = (P+Y).sum()\n",
    "\n",
    "# note that L requires_grad is False\n",
    "print(f'L requires_grad after computation whithin torch.no_grad():\\n{L.requires_grad}')\n",
    "print('\\n')\n",
    "\n",
    "# another way to disable gradient tracking is using the detach() method\n",
    "# this method returns a new Tensor, detached from the current graph\n",
    "L2 = Y.mean()\n",
    "print(f'requires_grad of L2\\n{L2.requires_grad}')\n",
    "L2_detach = L2.detach()\n",
    "print(f'requires_grad of L2.detach()\\n{L2_detach.requires_grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2809aabb",
   "metadata": {
    "id": "2809aabb"
   },
   "source": [
    "### Moving tensors to GPU and CPU memory\n",
    "A tensor created in the CPU can be moved to GPU, and vice-versa, using the method `torch.Tensor.to(device=None, dtype=None)`. This method provides parameters to perform tensor dtype and/or device conversion.\n",
    "\n",
    "Other ways to move tensors are:\n",
    "1) during construction by providing the device argument\n",
    "\n",
    "2) using the methods: `torch.Tensor.cpu()` or `torch.Tensor.cuda()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bdedc0e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2bdedc0e",
    "outputId": "930e2a78-a516-4bfa-8c69-200bbf806507"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor in cpu\n",
      "device: cpu\n",
      "Cuda is not available. Using CPU\n"
     ]
    }
   ],
   "source": [
    "# Returns a copy of the tensor in CPU/GPU memory\n",
    "\n",
    "# at instantiation time\n",
    "if torch.cuda.is_available():\n",
    "    D = torch.tensor([2.,3.,4.,5.], device = 'cuda')\n",
    "    print(f'Tensor in cuda')\n",
    "else:\n",
    "    D = torch.tensor([2.,3.,4.,5.], device = 'cpu')\n",
    "    print(f'Tensor in cpu')\n",
    "\n",
    "# using torch.Tensor.to(device) method\n",
    "if torch.cuda.is_available():\n",
    "    hdm = torch.device('cuda')\n",
    "else:\n",
    "    hdm = torch.device('cpu')\n",
    "print(f'device: {hdm}')\n",
    "D.to(hdm)\n",
    "\n",
    "# using torch.Tensor.cuda(device) method\n",
    "if torch.cuda.is_available():\n",
    "    D.cuda()\n",
    "    print(f'Tensor moved to cuda')\n",
    "else:\n",
    "    print(f'Cuda is not available. Using CPU')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf3648b",
   "metadata": {
    "id": "8bf3648b"
   },
   "source": [
    "### Saving and loading tensors\n",
    "Easily save and load tensors with the methods torch.save(torch.Tensor, file_name) and torch.load(file_name)\n",
    "\n",
    "PyTorch file extensions are '.pt' or '.pth'\n",
    "\n",
    "Learn more on serialization semantics at: https://pytorch.org/docs/stable/notes/serialization.html#saving-loading-tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146adef0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "146adef0",
    "outputId": "5b434a28-221b-4bb2-d52b-43b6bd36f7a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([[1,2],[3,4]])\n",
    "torch.save(y, 'tensor_file.pt')\n",
    "torch.load('tensor_file.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a552221e",
   "metadata": {
    "id": "a552221e"
   },
   "source": [
    "### A few other PyTorch.torch methods\n",
    "\n",
    "PyTorch offers a myriade (over 300) of methods to manipulate tensors. Here a few others:\n",
    "\n",
    "`torch.manual_seed(seed)` # sets the seed for generating random numbers. Returns a torch.Generator object.\n",
    "\n",
    "`torch.t(torch.Tensor)` # returns transpose dimensions 0 and 1 of a tensor of 2 dimesion or less\n",
    "\n",
    "`torch.transpose(torch.Tensor, dim0, dim1)` # returns a tensor that is a transposed version of input. The given dimensions dim0 and dim1 are swapped\n",
    "\n",
    "`torch.permute(torch.Tensor)` # returns a view of the input tensor with permuted dimension\n",
    "\n",
    "`torch.reshape(input, shape)` # returns a view of input tensor with specified shape\n",
    "\n",
    "`torch.select(torch.Tensor, dim, index)` # returns a view of input tensor as a slice along specified dimension and index\n",
    "\n",
    "`torch.split(torch.Tensor, split_size/split_sections, dim=0)` # returns chunks of the input tensor along the specified dimension. Each chunck is a view of original tensor\n",
    "\n",
    "`torch.stack(torch.Tensors, dim=0, out=None)` # concatenate a sequence of tensors along a given dimension\n",
    "\n",
    "`torch.eye(n, m=n)` # returns a 2-D tensor with ones on the diagonal and zeros elsewhere (n is number of rows)\n",
    "\n",
    "`torch.linalg.inv(A, *, out=None)`\n",
    "\n",
    "`torch.argmax(torch.Tensor)` # returns the index of element with the largest value in the input tensor\n",
    "\n",
    "`torch.sum(input, dtype=None)`\n",
    "\n",
    "`torch.pow(input, exponent, *, out=None)`\n",
    "\n",
    "`torch.div(input, other, out=None)` # divide each element of input tensor by the corresponding element in other tensor EQU\n",
    "\n",
    "`torch.add(input, other, alpha=1, out=None)` #add another tensor, or a scalar, scaled by alpha to the input tensor EQU\n",
    "\n",
    "`torch.no_grad()`/`torch.enable_grad()` #disable/enable gradient computation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876ad878",
   "metadata": {
    "id": "876ad878"
   },
   "source": [
    "### To learn more\n",
    "An extensive explanation of all the methods and their usage is available in PyTorch documentation: https://pytorch.org/docs/stable/torch.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc804166",
   "metadata": {
    "id": "bc804166"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
